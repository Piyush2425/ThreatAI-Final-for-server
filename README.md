# Threat-AI: Evidence-Based Threat Intelligence with Local LLM

**Production-ready threat intelligence analysis system with Ollama LLM, Chroma vector database, and web UI.**

![Status](https://img.shields.io/badge/status-ready-brightgreen) ![Python](https://img.shields.io/badge/python-3.10+-blue) ![Model](https://img.shields.io/badge/model-llama3%3A8b-success)

## ğŸš€ Quick Start

### 1. Prerequisites
```bash
# Ollama must be running
ollama serve
```

### 2. Start Web UI
```bash
python start_ui.py
```

Then open: **http://localhost:5000**

### 3. Interactive CLI (Alternative)
```bash
python app.py
```

## ğŸ“Š System Status

```
âœ… Data: 503 threat actors â†’ 1,267 semantic chunks
âœ… Vector DB: Chroma (8.9MB, <100ms search)
âœ… LLM: llama3:8b (Ollama, local inference)
âœ… Embeddings: 384-dim (sentence-transformers)
âœ… Web UI: Flask-based interface
âœ… Ready for testing
```

## ğŸ¯ Web UI Features

- **Query Interface**: Ask threat intelligence questions
- **Live Results**: Real-time analysis with evidence
- **Confidence Scoring**: 0-100% reliability metric
- **Evidence Display**: Cited sources with similarity scores
- **Sample Queries**: Pre-populated examples
- **System Status**: LLM mode, model info

## ğŸ“ Project Structure

```
threat-ai/
â”œâ”€â”€ web_ui.py              # Flask web server
â”œâ”€â”€ start_ui.py            # Startup script
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Web UI interface
â”œâ”€â”€ app.py                 # CLI application
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml      # Configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # 503 threat actor profiles
â”‚   â”œâ”€â”€ canonical/         # Normalized data
â”‚   â””â”€â”€ chroma_db/         # Vector database
â”œâ”€â”€ ingestion/             # Data loading
â”œâ”€â”€ chunking/              # Semantic segmentation
â”œâ”€â”€ embeddings/            # Vector generation
â”œâ”€â”€ retrieval/             # Semantic search
â”œâ”€â”€ agent/                 # LLM integration
â”‚   â”œâ”€â”€ interpreter.py     # Ollama integration âœ¨
â”‚   â””â”€â”€ guardrails.py      # Confidence scoring
â”œâ”€â”€ evaluation/            # Audit trails
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

**File:** `config/settings.yaml`

```yaml
ollama:
  model: llama3:8b          # Using 8B model
  host: http://localhost:11434
  timeout: 120
  temperature: 0.3
  max_tokens: 512
```

## ğŸ“ˆ Architecture

```
Query
  â†“
Vector Search (Chroma DB)  [<100ms]
  â†“
Evidence Formatting
  â†“
Ollama LLM (llama3:8b)     [1-5s]
  â†“
Confidence Scoring
  â†“
Response with Evidence
```

## ğŸ§ª Testing

**Test Web UI:**
```bash
python start_ui.py
# Navigate to http://localhost:5000
```

**Test CLI:**
```bash
python app.py
> Query: What are APT28 tactics?
```

**Integration Tests:**
```bash
python test_ollama.py      # Ollama integration
python test_chroma.py      # Vector store
```

## ğŸ“Š Performance

- Vector search: <100ms
- Ollama inference: 1-5s (llama3:8b)
- Full response: 5-10s
- Storage: 11.4MB total

## ğŸ” Security

âœ… Local processing (no cloud)
âœ… No API keys
âœ… Persistent storage on disk
âœ… Audit logging

## ğŸ“¦ Dependencies

```
chromadb>=0.4.0
sentence-transformers>=2.2.0
ollama>=0.1.0
requests>=2.28.0
flask>=2.0.0
pyyaml>=6.0
pydantic>=2.0.0
```

Install:
```bash
pip install -r requirements.txt
```

## ğŸ“ Documentation

- [SYSTEM_SUMMARY.md](SYSTEM_SUMMARY.md) - Architecture overview
- [OLLAMA_SETUP.md](OLLAMA_SETUP.md) - Ollama installation & configuration

## âœ¨ Key Features

- **Web UI**: Beautiful, responsive interface
- **Ollama LLM**: Local inference (llama3:8b)
- **Evidence Grounding**: Every answer cites sources
- **Confidence Scoring**: Reliability (0-100%)
- **Semantic Search**: <100ms vector retrieval
- **Modular Design**: Independent components
- **Error Handling**: Graceful degradation

## ğŸš€ Usage Examples

### Web UI Query
```
Query: What vulnerabilities does Lazarus Group exploit?

Response:
  Lazarus Group is known for exploiting zero-day vulnerabilities in...
  
Confidence: 84.5%

Evidence:
  [1] vulnerabilities (Score: 0.891) - Lazarus targets...
  [2] tactics (Score: 0.821) - Known for sophisticated...
```

### CLI Query
```bash
$ python app.py
> Query: Describe Emotet propagation
  Answer: Emotet is a polymorphic banking trojan that spreads...
  Confidence: 78.3%
  Sources: 3
```

## ğŸ“‹ Data Pipeline

1. **503 Threat Actors** â†’ Raw JSON
2. **Normalization** â†’ Standardized fields
3. **Chunking** â†’ 1,267 semantic segments
4. **Embedding** â†’ 384-dimensional vectors
5. **Storage** â†’ Chroma DB (persistent)
6. **Retrieval** â†’ Cosine similarity search
7. **LLM** â†’ Evidence-grounded answers

## ğŸ› ï¸ Troubleshooting

**Ollama not connecting:**
```bash
# Start Ollama
ollama serve

# Check model
ollama list
```

**Slow responses:**
- Check CPU usage
- Reduce `max_tokens` in settings.yaml
- Use `mistral` model instead

**No results from search:**
Lower `similarity_threshold` in settings.yaml:
```yaml
retrieval:
  similarity_threshold: 0.4  # Was 0.6
```

## ğŸ“š Model Information

**llama3:8b** (Current)
- Parameters: 8 billion
- RAM: 4.7GB
- Speed: 1-3s per query
- Quality: Excellent reasoning
- Recommended: Yes â­

## ğŸ¯ Next Steps

1. âœ… Web UI built
2. âœ… Ollama integrated
3. âœ… Testing ready
4. â§– Deploy to production
5. â§– Monitor performance
6. â§– Integrate threat feeds

## ğŸ“ Support

- [Ollama Docs](https://ollama.ai/docs)
- [Chroma Docs](https://docs.trychroma.com)
- [Flask Docs](https://flask.palletsprojects.com)

---

**Status:** âœ… Production Ready | **Version:** 1.0.0 | **Date:** 2026-01-30

**Start Now:** `python start_ui.py` â†’ Open http://localhost:5000










threat-ai/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml
â”‚   â””â”€â”€ logging.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ threat_actor_profiles.json
â”‚   â”‚
â”‚   â”œâ”€â”€ canonical/
â”‚   â”‚   â””â”€â”€ actors.json              # validated / normalized JSON
â”‚   â”‚
â”‚   â””â”€â”€ derived/
â”‚       â”œâ”€â”€ semantic_chunks.jsonl    # text chunks + metadata
â”‚       â””â”€â”€ chunk_schema.json
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ load_raw.py                  # load raw JSON
â”‚   â”œâ”€â”€ validate.py                  # schema validation
â”‚   â””â”€â”€ normalize.py                 # optional field normalization
â”‚
â”œâ”€â”€ chunking/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chunker.py                   # JSON â†’ semantic documents
â”‚   â””â”€â”€ rules.py                     # chunking rules (explicit)
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedder.py                  # local embeddings
â”‚   â””â”€â”€ vector_store.py              # FAISS or equivalent
â”‚
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ router.py                    # query â†’ retrieval plan
â”‚   â””â”€â”€ retrieve.py                  # evidence selection
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ system_prompt.txt            # evidence-grounded rules
â”‚   â”œâ”€â”€ interpreter.py               # LLM call (explain only)
â”‚   â””â”€â”€ guardrails.py                # confidence, uncertainty checks
â”‚
â”œâ”€â”€ feedback/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ schema.json
â”‚   â””â”€â”€ store.py                     # analyst feedback storage
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ confidence.py                # coverage & confidence scores
â”‚   â””â”€â”€ audit.py                     # traceability checks
â”‚
â”œâ”€â”€ app.py                           # CLI entrypoint (MVP)
â””â”€â”€ .gitignore




Query History (5-10% effort, high value)
Analytics Dashboard (15-20% effort, good insights)
Export to PDF/CSV (10% effort, essential)
Search Suggestions (10% effort, UX improvement)
RBAC/User System (20% effort, security critical)